{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import pickle\n",
    "import statistics\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print the number of users, bundles, items\n",
    "\n",
    "def get_counters(dataset_name):\n",
    "    print(\"\\n\" + dataset_name)\n",
    "    num_users, num_bundles, num_items = 0,0,0\n",
    "    with open(f\"..\\data\\{dataset_name}\\{dataset_name}_data_size.txt\") as f:        \n",
    "        num_users, num_bundles, num_items = [int(s) for s in f.readline().split('\\t')][:3]\n",
    "    print(f\"num users: {num_users}\")\n",
    "    print(f\"num_items: {num_items}\")\n",
    "    print(f\"num_bundles: {num_bundles}\")\n",
    "    return num_users, num_items, num_bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an item-item file of each item and its cooccurence items\n",
    "def create_cooccurences_file(dataset_name, num_items):\n",
    "    df = pd.read_csv(f\"..\\\\data\\\\{dataset_name}\\\\bundle_item.txt\", delimiter=\"\\t\", names=[\"bundle\", \"item\"])\n",
    "    with open(f\"..\\\\data\\\\{dataset_name}\\\\item_item.txt\", 'w') as output_file:\n",
    "        for item in range(num_items):\n",
    "            bundles_for_item = set(df[df[\"item\"] == item][\"bundle\"])\n",
    "            cooccuring_items = set(df[df[\"bundle\"].apply(lambda b: b in bundles_for_item)][\"item\"])\n",
    "            for citem in cooccuring_items:\n",
    "                if citem != item:\n",
    "                    output_file.write(f\"{item}\\t{citem}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take subset of the positive items and create a negative matches for them\n",
    "def Create_negative_item_item_file_subset(dataset_name, total_num_items, num_pos_interactions_to_take):\n",
    "    netease_item_item = pd.read_csv(os.path.join(\"..\",\"data\", dataset_name, 'item_item.txt'), delimiter=\"\\t\", \n",
    "                                    names=[\"item1\", \"item2\"])\n",
    "    num_positive_samples = len(netease_item_item)\n",
    "    fix_seed(123)\n",
    "    \n",
    "    chosen_positive_interactions = np.random.choice(range(num_positive_samples), num_pos_interactions_to_take, replace=True)\n",
    "    chosen_positive_interactions.sort()\n",
    "    small_netease_item_item = netease_item_item.iloc[chosen_positive_interactions]\n",
    "    \n",
    "    negative_items = np.random.choice(range(total_num_items), num_pos_interactions_to_take, replace=True)\n",
    "    item1_list = small_netease_item_item[\"item1\"].values\n",
    "    item2_list = small_netease_item_item[\"item2\"].values\n",
    "    \n",
    "    latest_item1_index = -1\n",
    "    latest_positives = set()\n",
    "    for i in tqdm(range(num_pos_interactions_to_take)):\n",
    "        current_item1_index = item1_list[i]\n",
    "        if current_item1_index == latest_item1_index:\n",
    "            item_positive_items = latest_positives\n",
    "        else:\n",
    "            item_positive_items = set(netease_item_item[netease_item_item[\"item1\"] == current_item1_index][\"item2\"])\n",
    "            latest_item1_index = current_item1_index\n",
    "            latest_positives = item_positive_items.copy()\n",
    "        while (negative_items[i] == current_item1_index or negative_items[i] in item_positive_items):\n",
    "            negative_items[i] = np.random.choice(range(total_num_items))\n",
    "\n",
    "    with open(f\"..\\\\data\\\\{dataset_name}\\\\item_item_negatives_10M.txt\", 'w') as output_file:\n",
    "        for item1, neg in zip(item1_list, negative_items):\n",
    "            output_file.write(f\"{item1.item()}\\t{neg.item()}\\n\")\n",
    "            \n",
    "    with open(f\"..\\\\data\\\\{dataset_name}\\\\item_item_positives_10M.txt\", 'w') as output_file:\n",
    "        for item1, pos in zip(item1_list, item2_list):\n",
    "            output_file.write(f\"{item1.item()}\\t{pos.item()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas version - all items\n",
    "def Create_negative_item_item_file(dataset_name, total_num_items):\n",
    "    netease_item_item = pd.read_csv(os.path.join(\"..\",\"data\", dataset_name, 'item_item.txt'), delimiter=\"\\t\", names=[\"item1\", \"item2\"])\n",
    "    num_positive_samples = len(netease_item_item)\n",
    "    fix_seed(123)\n",
    "    negative_items = np.random.choice(range(total_num_items), num_positive_samples, replace=True)\n",
    "    item1_list = netease_item_item[\"item1\"]\n",
    "    latest_item1_index = -1\n",
    "    latest_positives = set()\n",
    "    for i in tqdm(range(num_positive_samples)):\n",
    "        current_item1_index = item1_list[i]\n",
    "        if current_item1_index == latest_item1_index:\n",
    "            item_positive_items = latest_positives\n",
    "        else:\n",
    "            item_positive_items = set(netease_item_item[netease_item_item[\"item1\"] == current_item1_index][\"item2\"])\n",
    "            latest_item1_index = current_item1_index\n",
    "            latest_positives = item_positive_items.copy()\n",
    "        while negative_items[i] in item_positive_items or negative_items[i] == current_item1_index:\n",
    "            negative_items[i] = np.random.choice(range(total_num_items))\n",
    "\n",
    "    with open(f\"..\\\\data\\\\{dataset_name}\\\\item_item_negatives3.txt\", 'w') as output_file:\n",
    "        for item1, neg in zip(item1_list, negative_items):\n",
    "            output_file.write(f\"{item1.item()}\\t{neg.item()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSR version - all items\n",
    "def Create_negative_item_item_file(dataset_name, total_num_items):\n",
    "    with open(os.path.join(\"..\",\"data\", dataset_name, 'item_item.txt'), 'r') as f:\n",
    "        items_paires = list(map(lambda s: tuple(int(i) for i in s[:-1].split('\\t')), f.readlines()))\n",
    "    indice = np.array(items_paires, dtype=np.int32)\n",
    "    values = np.ones(len(items_paires), dtype=np.float32)\n",
    "    item_item_metrix = sp.coo_matrix((values, (indice[:, 0], indice[:, 1])), shape=(total_num_items, total_num_items)).tocsr()\n",
    "\n",
    "    item1_list, positive_items = item_item_metrix.nonzero()\n",
    "    num_positive_samples = item1_list.shape[0]\n",
    "    # A negative item2 for each item\n",
    "    negative_items = np.random.choice(range(total_num_items), num_positive_samples, replace=True)\n",
    "\n",
    "    for i in tqdm(range(num_positive_samples)):\n",
    "        left_item_index = item1_list[i]\n",
    "        item_positive_items = set(item_item_metrix[left_item_index].nonzero()[1])\n",
    "        while negative_items[i] in item_positive_items or negative_items[i] == left_item_index:\n",
    "            negative_items[i] = np.random.choice(range(total_num_items))\n",
    "\n",
    "    with open(f\"..\\\\data\\\\{dataset_name}\\\\item_item_negatives2.txt\", 'w') as output_file:\n",
    "        for item1, neg in zip(item1_list, negative_items):\n",
    "            output_file.write(f\"{item1.item()}\\t{neg.item()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coocurrences_per_item(dataset_name)\n",
    "    item_item = pd.read_csv(os.path.join(\"..\",\"data\", dataset_name, 'item_item.txt'), delimiter=\"\\t\", \n",
    "                                    names=[\"item1\", \"item2\"])\n",
    "    counts = item_item.groupby(\"item1\").count()\n",
    "    coutne.\n",
    "    = len(netease_item_item)\n",
    "    fix_seed(123)\n",
    "    \n",
    "    chosen_positive_interactions = np.random.choice(range(num_positive_samples), num_pos_interactions_to_take, replace=True)\n",
    "    chosen_positive_interactions.sort()\n",
    "    small_netease_item_item = netease_item_item.iloc[chosen_positive_interactions]\n",
    "    \n",
    "    negative_items = np.random.choice(range(total_num_items), num_pos_interactions_to_take, replace=True)\n",
    "    item1_list = small_netease_item_item[\"item1\"].values\n",
    "    item2_list = small_netease_item_item[\"item2\"].values\n",
    "    \n",
    "    latest_item1_index = -1\n",
    "    latest_positives = set()\n",
    "    for i in tqdm(range(num_pos_interactions_to_take)):\n",
    "        current_item1_index = item1_list[i]\n",
    "        if current_item1_index == latest_item1_index:\n",
    "            item_positive_items = latest_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item1</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2812</th>\n",
       "      <td>2814</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2813</th>\n",
       "      <td>2815</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>2816</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815</th>\n",
       "      <td>2817</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2816</th>\n",
       "      <td>2818</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2817 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      item1  counts\n",
       "0         0       2\n",
       "1         1       1\n",
       "2         2       1\n",
       "3         3      47\n",
       "4         4      21\n",
       "...     ...     ...\n",
       "2812   2814       1\n",
       "2813   2815       2\n",
       "2814   2816       1\n",
       "2815   2817      23\n",
       "2816   2818      87\n",
       "\n",
       "[2817 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetEase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████▋                                                      | 32640894/237323182 [9:19:19<63:53:55, 889.79it/s]"
     ]
    }
   ],
   "source": [
    "#for dataset_name in [\"Steam\", \"NetEase\", \"Youshu\"]:\n",
    "#for dataset_name in [\"NetEase\"]:\n",
    "#    print(dataset_name)\n",
    "#    num_items = counters_dict[dataset_name][1]\n",
    "#    Create_negative_item_item_file(dataset_name, num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data to 80,10,10\n",
    "def split_train_val_test(dataset_name):\n",
    "    print(dataset_name)\n",
    "    fix_seed(123)\n",
    "    pos_df = pd.read_csv(f\"../data/{dataset_name}/item_item_positives_10M.txt\", delimiter=\"\\t\", names=[\"bundle\", \"item\"])\n",
    "    pos_df[\"label\"] = 1\n",
    "    neg_df = pd.read_csv(f\"../data/{dataset_name}/item_item_negatives_10M.txt\", delimiter=\"\\t\", names=[\"bundle\", \"item\"])\n",
    "    neg_df[\"label\"] = 0\n",
    "    \n",
    "    num_total_pos = len(pos_df)\n",
    "    test_and_val_size = 60000 if num_total_pos * 0.2 > 60000 else 0.2\n",
    "    \n",
    "    # take equal pos and negs (80,10,10)\n",
    "    pos_train, pos_test = train_test_split(pos_df, test_size=test_and_val_size)\n",
    "    neg_train, neg_test = train_test_split(neg_df, test_size=test_and_val_size)\n",
    "    pos_val, pos_test = train_test_split(pos_test, test_size=0.5)\n",
    "    neg_val, neg_test = train_test_split(neg_test, test_size=0.5)\n",
    "    \n",
    "    # unite pos and neg\n",
    "    train = pd.concat([pos_train, neg_train])\n",
    "    val = pd.concat([pos_val, neg_val])\n",
    "    test = pd.concat([pos_test, neg_test])\n",
    "\n",
    "    # To file\n",
    "    train.to_csv(f\"../data/{dataset_name}/item_item_train.txt\", sep=\"\\t\", header=False, index=False)\n",
    "    val.to_csv(f\"../data/{dataset_name}/item_item_val.txt\", sep=\"\\t\", header=False, index=False)\n",
    "    test.to_csv(f\"../data/{dataset_name}/item_item_test.txt\", sep=\"\\t\", header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}